# -*- coding: utf-8 -*-
"""Identifying Principal components .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vr-RdMebe0byEcziPnSYStpyMz0q_C91
"""

!pip install pyspark

from pyspark.sql import SparkSession  #Iimporting spark session.
spark = SparkSession.builder.appName("bonus").getOrCreate()
df = spark.read.csv('/content/original_data.csv',header =True, inferSchema=True)

from pyspark.ml.feature import PCA as PCAml
from pyspark.ml.linalg import Vectors

df_bonus=df.select(df['researchExp'],df['industryExp'],df['cgpa'],df['admit'])
df_bonus.dtypes

from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
df_bonus.drop("researchExp_Scaled","admit_scaled")
df_bonus.show()

unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())

for i in ["researchExp","industryExp","cgpa","admit"]:
    assembler = VectorAssembler(inputCols=[i],outputCol=i+"_Vect")
    scaler = MinMaxScaler(inputCol=i+"_Vect", outputCol=i+"_Scaled")
    pipeline = Pipeline(stages=[assembler, scaler])
    df1 = pipeline.fit(df_bonus).transform(df_bonus).withColumn(i+"_Scaled", unlist(i+"_Scaled")).drop(i+"_Vect")

df1.show()

for i in ["researchExp","industryExp","cgpa",]:
    assembler = VectorAssembler(inputCols=[i],outputCol=i+"_Vect")
    scaler = MinMaxScaler(inputCol=i+"_Vect", outputCol=i+"_Scaled")
    pipeline = Pipeline(stages=[assembler, scaler])
    df1 = pipeline.fit(df1).transform(df1).withColumn(i+"_Scaled", unlist(i+"_Scaled")).drop(i+"_Vect")



df1.show()

df1.drop("researchExp","industryExp","cgpa","admit")

df1=df1.select("researchExp_Scaled","industryExp_Scaled","cgpa_Scaled","admit_Scaled")

df1.show()

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=df1.columns[1:], outputCol="variable")
feature_vectors = assembler.transform(df1)
feature_vectors.show(truncate=False)

from pyspark.ml.feature import StandardScaler
scaler = StandardScaler(inputCol="variable", outputCol="final_variable", withStd=True, withMean=True)
scalerModel = scaler.fit(feature_vectors)
std_feature_vectors = scalerModel.transform(feature_vectors)
#print("====Standardized data====")
#std_feature_vectors.select("final_variable").show(truncate=False)

from pyspark.ml.feature import PCA

pca = PCA(k=3, inputCol="variable", outputCol="Main component score")
pcaModel = pca.fit(feature_vectors)

print("====Eigenvector====")
print(pcaModel.pc)

print("====Contribution rate====")
print(pcaModel.explainedVariance)

pca_score = pcaModel.transform(std_feature_vectors).select("Main component score")
print("====Main component score====")
pca_score.show(truncate=False)

#references:- https://linuxtut.com/en/7714a1cc9be56588b72a/
# https://stackoverflow.com/questions/40337744/scalenormalise-a-column-in-spark-dataframe-pyspark