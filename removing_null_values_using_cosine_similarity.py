# -*- coding: utf-8 -*-
"""removing null values using cosine similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SHVVJDbMNdRZDX4z71KZfKENWIVCmdzp
"""

!pip install spark

!pip install pyspark
from pyspark.sql import SparkSession  #Iimporting spark session.
spark = SparkSession.builder.appName("Task1").getOrCreate()
df = spark.read.csv('/content/original_data.csv',header =True, inferSchema=True)

from pyspark.sql import functions as F
cols = [f"any({col} is null) as {col}" for col in df.columns]
df.selectExpr(cols).show()

df.dtypes

#1)major -string
#2)specialization-string
#3)toeflScore-int
#4)program-string
#5)department-string
#6)toeflEssay-string
#7)internExp-string
#8)greV-int
#9)greQ-int
#10)journalPubs-string
#11)greA-double
#12)topperCgpa-double
#13)termAndYear-string
#14)confPubs-string
#15)ugCollege-string
#16)gmatA-int
#17)gmatQ-int             #these are columns that are having null values.
#18)gmatV-int

#https://tipsfordev.com/calculating-the-cosine-similarity-between-all-the-rows-of-a-dataframe-in-pyspark

df.show()

import pyspark.sql.functions as func

def cosine_similarity(df, col1, col2):
    df_cosine = df.select(func.sum(df[col1] * df[col2]).alias('dot'), 
                          func.sqrt(func.sum(df[col1]**2)).alias('norm1'), 
                          func.sqrt(func.sum(df[col2] **2)).alias('norm2'))
    d = df_cosine.rdd.collect()[0].asDict()
    return d['dot']/(d['norm1'] * d['norm2'])

x=cosine_similarity(df, 'industryExp', 'admit')
cache=[]
cache.append(x)
y=cosine_similarity(df,'researchExp','industryExp')
cache.append(y)
z=cosine_similarity(df,'researchExp','admit')
cache.append(z)
print(cache)

#here in this cosine similarity values combination between admit and industryExp gave highest value which means I need to fill based on either of the two columns.

print("the maximum cosine similarity found observed is between columns 'industryExp','admit' with value",cache[0])

df1=df.select(df['greA'],df['greV'],df['greQ'])
df1.describe(['greA']).show()

from pyspark.sql.types import NullType
from pyspark.sql.functions import isnan, when, count, col, lit
from pyspark.sql.window import Window
import sys

df = df.withColumn('greA', when(df.greA<0, lit(None)).when(df.greA>1470.0, lit(None)).otherwise(df.greA))
df= df.withColumn("greA", func.last('greA', True).over(Window.partitionBy('admit').rowsBetween(-sys.maxsize, 0)))



df1.describe(['greV']).show()

df = df.withColumn('greV', when(df.greV<0, lit(None)).when(df.greV>5560, lit(None)).otherwise(df.greV))
df= df.withColumn("greV", func.last('greV', True).over(Window.partitionBy('admit').rowsBetween(-sys.maxsize, 0)))

df1.describe(['greq']).show()

df = df.withColumn('greQ', when(df.greQ<0, lit(None)).when(df.greQ>7990, lit(None)).otherwise(df.greQ))
df= df.withColumn("greQ", func.last('greQ', True).over(Window.partitionBy('admit').rowsBetween(-sys.maxsize, 0)))

df1=df.select(df['greA'],df['greV'],df['greQ'])

df1.select([count(when(isnan(c)| col(c).isNull(), c)).alias(c) for c in df1.columns]).show()

df.select([count(when( isnan(c)|col(c).isNull(), c)).alias(c) for c in df.columns]).show()

#references :-https://sparkbyexamples.com/pyspark/pyspark-window-functions/